{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FR-ar_4krOl",
        "colab_type": "text"
      },
      "source": [
        "### Load datas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voKu6gPPiM3j",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "997d9ad5-32dc-428b-f137-6465a84121c3"
      },
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-118c79dd-c62c-4baa-8bd6-89fac3ceb121\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-118c79dd-c62c-4baa-8bd6-89fac3ceb121\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_test_3.txt to data_test_3 (2).txt\n",
            "Saving data_train_3.csv to data_train_3.csv\n",
            "Saving data_train_7.csv to data_train_7 (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3nn9hBQmh9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_values = {\n",
        "  \"neutral\": 0,\n",
        "  \"positive\": 1,\n",
        "  \"negative\": -1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2om5EbLj8Dg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9b4df079-ca55-4d3a-8dfc-b5ca727465d5"
      },
      "source": [
        "with open('data_test_3.txt', 'r', encoding='utf-8') as f:\n",
        "    df_test_3 = pd.DataFrame([s.split('\\t') for s in f.read().split('\\n')[:-1]], columns=['id', 'sentiment', 'tweet', 'a'])\n",
        "    df_test_3 = df_test_3.drop(columns=['a'])\n",
        "    df_test_3['sentiment'] = df_test_3['sentiment'].map(sentiment_values)\n",
        "df_test_3.head()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>264221473558917120</td>\n",
              "      <td>0</td>\n",
              "      <td>Manchester United will try to return to winnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>264091690632105984</td>\n",
              "      <td>0</td>\n",
              "      <td>Going to a bulls game with Aaliyah &amp; hope next...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>263929564907069441</td>\n",
              "      <td>0</td>\n",
              "      <td>Any Toon Fans with a spare ticket for Anfield ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>263759328782204928</td>\n",
              "      <td>1</td>\n",
              "      <td>Louis inspired outfit on Monday and Zayn inspi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>259546192722161664</td>\n",
              "      <td>0</td>\n",
              "      <td>going to bed now...Rose parade then game tomorrow</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id  ...                                              tweet\n",
              "0  264221473558917120  ...  Manchester United will try to return to winnin...\n",
              "1  264091690632105984  ...  Going to a bulls game with Aaliyah & hope next...\n",
              "2  263929564907069441  ...  Any Toon Fans with a spare ticket for Anfield ...\n",
              "3  263759328782204928  ...  Louis inspired outfit on Monday and Zayn inspi...\n",
              "4  259546192722161664  ...  going to bed now...Rose parade then game tomorrow\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKWjGt9tlWaW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dcf004e4-e048-48e1-b478-832ba8158317"
      },
      "source": [
        "df_train_3 = pd.read_csv('data_train_3.csv', sep='\\t', encoding='utf-8', names=['id', 'sentiment', 'tweet'])\n",
        "df_train_3['sentiment'] = df_train_3['sentiment'].map(sentiment_values)\n",
        "df_train_3.head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>260097528899452929</td>\n",
              "      <td>0</td>\n",
              "      <td>Won the match #getin . Plus\\u002c tomorrow is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>263791921753882624</td>\n",
              "      <td>0</td>\n",
              "      <td>Some areas of New England could see the first ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>264194578381410304</td>\n",
              "      <td>-1</td>\n",
              "      <td>@francesco_con40 2nd worst QB. DEFINITELY Tony...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>264041328420204544</td>\n",
              "      <td>0</td>\n",
              "      <td>#Thailand Washington - US President Barack Oba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>263816256640126976</td>\n",
              "      <td>0</td>\n",
              "      <td>Did y\\u2019all hear what Tony Romo dressed up ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id  ...                                              tweet\n",
              "0  260097528899452929  ...  Won the match #getin . Plus\\u002c tomorrow is ...\n",
              "1  263791921753882624  ...  Some areas of New England could see the first ...\n",
              "2  264194578381410304  ...  @francesco_con40 2nd worst QB. DEFINITELY Tony...\n",
              "3  264041328420204544  ...  #Thailand Washington - US President Barack Oba...\n",
              "4  263816256640126976  ...  Did y\\u2019all hear what Tony Romo dressed up ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKzpNIm9lZ5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d99dc8e2-ae2a-4f68-cda6-e14a971b8914"
      },
      "source": [
        "df_train_7 = pd.read_csv('data_train_7.csv', sep='\\t', encoding='utf-8', names=['id', 'sentiment', 'tweet'])\n",
        "df_train_7.head()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@liamch88 yeah! :) playing well</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>At least I don't have a guy trying to discoura...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>UPLIFT: If you're still discouraged it means y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>...at your age, the heyday in the blood is tam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-2</td>\n",
              "      <td>i was so embarrassed when she saw us i was lik...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  sentiment                                              tweet\n",
              "0   0          0                   @liamch88 yeah! :) playing well \n",
              "1   1          0  At least I don't have a guy trying to discoura...\n",
              "2   2          0  UPLIFT: If you're still discouraged it means y...\n",
              "3   3          0  ...at your age, the heyday in the blood is tam...\n",
              "4   4         -2  i was so embarrassed when she saw us i was lik..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2icev7heuvh",
        "colab_type": "code",
        "outputId": "8e976c51-b5dc-4f62-a5e2-bbad4638cb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Length of train_3: %d' % (len(df_train_3)))\n",
        "print('Length of test_3: %d' % (len(df_test_3)))\n",
        "print('Length of train_7: %d' % (len(df_train_7)))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_3: 50333\n",
            "Length of test_3: 1020\n",
            "Length of train_7: 1630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxcPAmLeky2R",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UkKIqPexzY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
        "\n",
        "\"\"\"\n",
        "Emoticons and Emoji data dictonary\n",
        "\"\"\"\n",
        "\n",
        "emojis = {\n",
        "    u\":‑\\)\":\"☺️\",\n",
        "    u\":\\)\":\"☺️\",\n",
        "    u\":-\\]\":\"☺️\",\n",
        "    u\":\\]\":\"☺️\",\n",
        "    u\":-3\":\"☺️\",\n",
        "    u\":3\":\"☺️\",\n",
        "    u\":->\":\"☺️\",\n",
        "    u\":>\":\"☺️\",\n",
        "    u\"8-\\)\":\"☺️\",\n",
        "    u\":o\\)\":\"☺️\",\n",
        "    u\":-\\}\":\"☺️\",\n",
        "    u\":\\}\":\"☺️\",\n",
        "    u\":-\\)\":\"☺️\",\n",
        "    u\":c\\)\":\"☺️\",\n",
        "    u\":\\^\\)\":\"☺️\",\n",
        "    u\"=\\]\":\"☺️\",\n",
        "    u\"=\\)\":\"☺️\",\n",
        "    u\":‑D\":\"😃\",\n",
        "    u\":D\":\"😃\",\n",
        "    u\"8‑D\":\"😃\",\n",
        "    u\"8D\":\"😃\",\n",
        "    u\"X‑D\":\"😃\",\n",
        "    u\"XD\":\"😃\",\n",
        "    u\"=D\":\"😃\",\n",
        "    u\"=3\":\"😃\",\n",
        "    u\"B\\^D\":\"😃\",\n",
        "    u\":-\\)\\)\":\"😃\",\n",
        "    u\":‑\\(\":\"☹️\",\n",
        "    u\":-\\(\":\"☹️\",\n",
        "    u\":\\(\":\"☹️\",\n",
        "    u\":‑c\":\"☹️\",\n",
        "    u\":c\":\"☹️\",\n",
        "    u\":‑<\":\"☹️\",\n",
        "    u\":<\":\"☹️\",\n",
        "    u\":‑\\[\":\"☹️\",\n",
        "    u\":\\[\":\"☹️\",\n",
        "    u\":-\\|\\|\":\"☹️\",\n",
        "    u\">:\\[\":\"☹️\",\n",
        "    u\":\\{\":\"☹️\",\n",
        "    u\":@\":\"☹️\",\n",
        "    u\">:\\(\":\"☹️\",\n",
        "    u\":'‑\\(\":\"😭\",\n",
        "    u\":'\\(\":\"😭\",\n",
        "    u\":'‑\\)\":\"😃\",\n",
        "    u\":'\\)\":\"😃\",\n",
        "    u\"D‑':\":\"😨\",\n",
        "    u\"D:<\":\"😨\",\n",
        "    u\"D:\":\"😧\",\n",
        "    u\"D8\":\"😧\",\n",
        "    u\"D;\":\"😧\",\n",
        "    u\"D=\":\"😧\",\n",
        "    u\"DX\":\"😧\",\n",
        "    u\":‑O\":\"😮\",\n",
        "    u\":O\":\"😮\",\n",
        "    u\":‑o\":\"😮\",\n",
        "    u\":o\":\"😮\",\n",
        "    u\":-0\":\"😮\",\n",
        "    u\"8‑0\":\"😮\",\n",
        "    u\">:O\":\"😮\",\n",
        "    u\":-\\*\":\"😗\",\n",
        "    u\":\\*\":\"😗\",\n",
        "    u\":X\":\"😗\",\n",
        "    u\";‑\\)\":\"😉\",\n",
        "    u\";\\)\":\"😉\",\n",
        "    u\"\\*-\\)\":\"😉\",\n",
        "    u\"\\*\\)\":\"😉\",\n",
        "    u\";‑\\]\":\"😉\",\n",
        "    u\";\\]\":\"😉\",\n",
        "    u\";\\^\\)\":\"😉\",\n",
        "    u\":‑,\":\"😉\",\n",
        "    u\";D\":\"😉\",\n",
        "    u\":‑P\":\"😛\",\n",
        "    u\":P\":\"😛\",\n",
        "    u\"X‑P\":\"😛\",\n",
        "    u\"XP\":\"😛\",\n",
        "    u\":‑Þ\":\"😛\",\n",
        "    u\":Þ\":\"😛\",\n",
        "    u\":b\":\"😛\",\n",
        "    u\"d:\":\"😛\",\n",
        "    u\"=p\":\"😛\",\n",
        "    u\">:P\":\"😛\",\n",
        "    u\":‑/\":\"😕\",\n",
        "    u\":/\":\"😕\",\n",
        "    u\":-[.]\":\"😕\",\n",
        "    u\">:[(\\\\\\)]\":\"😕\",\n",
        "    u\">:/\":\"😕\",\n",
        "    u\":[(\\\\\\)]\":\"😕\",\n",
        "    u\"=/\":\"😕\",\n",
        "    u\"=[(\\\\\\)]\":\"😕\",\n",
        "    u\":L\":\"😕\",\n",
        "    u\"=L\":\"😕\",\n",
        "    u\":S\":\"😕\",\n",
        "    u\":‑\\|\":\"😐\",\n",
        "    u\":\\|\":\"😐\",\n",
        "    u\":$\":\"😳\",\n",
        "    u\":‑x\":\"🤐\",\n",
        "    u\":x\":\"🤐\",\n",
        "    u\":‑#\":\"🤐\",\n",
        "    u\":#\":\"🤐\",\n",
        "    u\":‑&\":\"🤐\",\n",
        "    u\":&\":\"🤐\",\n",
        "    u\"O:‑\\)\":\"😇\",\n",
        "    u\"O:\\)\":\"😇\",\n",
        "    u\"0:‑3\":\"😇\",\n",
        "    u\"0:3\":\"😇\",\n",
        "    u\"0:‑\\)\":\"😇\",\n",
        "    u\"0:\\)\":\"😇\",\n",
        "    u\":‑b\":\"😛\",\n",
        "    u\"0;\\^\\)\":\"😇\",\n",
        "    u\">:‑\\)\":\"😈\",\n",
        "    u\">:\\)\":\"😈\",\n",
        "    u\"\\}:‑\\)\":\"😈\",\n",
        "    u\"\\}:\\)\":\"😈\",\n",
        "    u\"3:‑\\)\":\"😈\",\n",
        "    u\"3:\\)\":\"😈\",\n",
        "    u\">;\\)\":\"😈\",\n",
        "    u\"\\|;‑\\)\":\"😎\",\n",
        "    u\"\\|‑O\":\"😏\",\n",
        "    u\":‑J\":\"😏\",\n",
        "    u\"%‑\\)\":\"😵\",\n",
        "    u\"%\\)\":\"😵\",\n",
        "    u\":-###..\":\"🤒\",\n",
        "    u\":###..\":\"🤒\",\n",
        "    u\"\\(>_<\\)\":\"😣\",\n",
        "    u\"\\(>_<\\)>\":\"😣\",\n",
        "    u\"\\(';'\\)\":\"👶\",\n",
        "    u\"\\(\\^\\^>``\":\"😓\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"😓\",\n",
        "    u\"\\(-_-;\\)\":\"😓\",\n",
        "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"😓\",\n",
        "    u\"\\(-_-\\)zzz\":\"😴\",\n",
        "    u\"\\(\\^_-\\)\":\"😉\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"😕\",\n",
        "    u\"\\(\\+o\\+\\)\":\"😕\",\n",
        "    u\"\\^_\\^\":\"😃\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"😃\",\n",
        "    u\"\\(\\^O\\^\\)／\":\"😃\",\n",
        "    u\"\\(\\^o\\^\\)／\":\"😃\",\n",
        "    u\"\\(__\\)\":\"🙇\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"🙇\",\n",
        "    u\"<\\(_ _\\)>\":\"🙇\",\n",
        "    u\"<m\\(__\\)m>\":\"🙇\",\n",
        "    u\"m\\(__\\)m\":\"🙇\",\n",
        "    u\"m\\(_ _\\)m\":\"🙇\",\n",
        "    u\"\\('_'\\)\":\"😭\",\n",
        "    u\"\\(/_;\\)\":\"😭\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"😭\",\n",
        "    u\"\\(;_;\":\"😭\",\n",
        "    u\"\\(;_:\\)\":\"😭\",\n",
        "    u\"\\(;O;\\)\":\"😭\",\n",
        "    u\"\\(:_;\\)\":\"😭\",\n",
        "    u\"\\(ToT\\)\":\"😭\",\n",
        "    u\";_;\":\"😭\",\n",
        "    u\";-;\":\"😭\",\n",
        "    u\";n;\":\"😭\",\n",
        "    u\";;\":\"😭\",\n",
        "    u\"Q\\.Q\":\"😭\",\n",
        "    u\"T\\.T\":\"😭\",\n",
        "    u\"QQ\":\"😭\",\n",
        "    u\"Q_Q\":\"😭\",\n",
        "    u\"\\(-\\.-\\)\":\"😞\",\n",
        "    u\"\\(-_-\\)\":\"😞\",\n",
        "    u\"\\(一一\\)\":\"😞\",\n",
        "    u\"\\(；一_一\\)\":\"😞\",\n",
        "    u\"\\(=_=\\)\":\"😩\",\n",
        "    u\"\\(=\\^\\·\\^=\\)\":\"😺\",\n",
        "    u\"\\(=\\^\\·\\·\\^=\\)\":\"😺\",\n",
        "    u\"=_\\^=\t\":\"😺\",\n",
        "    u\"\\(\\.\\.\\)\":\"😔\",\n",
        "    u\"\\(\\._\\.\\)\":\"😔\",\n",
        "    u\"\\(\\・\\・?\":\"😕\",\n",
        "    u\"\\(?_?\\)\":\"😕\",\n",
        "    u\">\\^_\\^<\":\"😃\",\n",
        "    u\"<\\^!\\^>\":\"😃\",\n",
        "    u\"\\^/\\^\":\"😃\",\n",
        "    u\"\\（\\*\\^_\\^\\*）\" :\"😃\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"😃\",\n",
        "    u\"\\(^\\^\\)\":\"😃\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"😃\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"😃\",\n",
        "    u\"\\(\\^_\\^\\)\":\"😃\",\n",
        "    u\"\\(\\^\\^\\)\":\"😃\",\n",
        "    u\"\\(\\^J\\^\\)\":\"😃\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"😃\",\n",
        "    u\"\\(\\^—\\^\\）\":\"😃\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"😃\",\n",
        "    u\"\\（\\^—\\^\\）\":\"👋\",\n",
        "    u\"\\(;_;\\)/~~~\":\"👋\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"👋\",\n",
        "    u\"\\(T_T\\)/~~~\":\"👋\",\n",
        "    u\"\\(ToT\\)/~~~\":\"👋\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"😍\",\n",
        "    u\"\\(\\*_\\*\\)\":\"😍\",\n",
        "    u\"\\(\\*_\\*;\":\"😍\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"😍\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"😂\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"😂\",\n",
        "    u'\\(-\"-\\)':\"😓\",\n",
        "    u\"\\(ーー;\\)\":\"😓\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"😎\",\n",
        "    u\"\\(\\＾ｖ\\＾\\)\":\"😀\",\n",
        "    u\"\\(\\＾ｕ\\＾\\)\":\"😀\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"😀\",\n",
        "    u\"\\(\\^O\\^\\)\":\"😀\",\n",
        "    u\"\\(\\^o\\^\\)\":\"😀\",\n",
        "    u\"\\)\\^o\\^\\(\":\"😀\",\n",
        "    u\":O o_O\":\"😮\",\n",
        "    u\"o_0\":\"😮\",\n",
        "    u\"o\\.O\":\"😮\",\n",
        "    u\"\\(o\\.o\\)\":\"😮\",\n",
        "    u\"oO\":\"😮\",\n",
        "    u\"\\(\\*￣m￣\\)\":\"😠\",\n",
        "    u\":‑)\":\"☺️\",\n",
        "    u\":)\":\"☺️\",\n",
        "    u\":-]\":\"☺️\",\n",
        "    u\":]\":\"☺️\",\n",
        "    u\":-3\":\"☺️\",\n",
        "    u\":3\":\"☺️\",\n",
        "    u\":->\":\"☺️\",\n",
        "    u\":>\":\"☺️\",\n",
        "    u\"8-)\":\"☺️\",\n",
        "    u\":o)\":\"☺️\",\n",
        "    u\":-}\":\"☺️\",\n",
        "    u\":}\":\"☺️\",\n",
        "    u\":-)\":\"☺️\",\n",
        "    u\":c)\":\"☺️\",\n",
        "    u\":^)\":\"☺️\",\n",
        "    u\"=]\":\"☺️\",\n",
        "    u\"=)\":\"☺️\",\n",
        "    u\":‑D\":\"😃\",\n",
        "    u\":D\":\"😃\",\n",
        "    u\"8‑D\":\"😃\",\n",
        "    u\"8D\":\"😃\",\n",
        "    u\"X‑D\":\"😃\",\n",
        "    u\"XD\":\"😃\",\n",
        "    u\"=D\":\"😃\",\n",
        "    u\"=3\":\"😃\",\n",
        "    u\"B^D\":\"😃\",\n",
        "    u\":-))\":\"😃\",\n",
        "    u\":-(\":\"☹️\",\n",
        "    u\":‑(\":\"☹️\",\n",
        "    u\":(\":\"☹️\",\n",
        "    u\":‑c\":\"☹️\",\n",
        "    u\":c\":\"☹️\",\n",
        "    u\":‑<\":\"☹️\",\n",
        "    u\":<\":\"☹️\",\n",
        "    u\":‑[\":\"☹️\",\n",
        "    u\":[\":\"☹️\",\n",
        "    u\":-||\":\"☹️\",\n",
        "    u\">:[\":\"☹️\",\n",
        "    u\":{\":\"☹️\",\n",
        "    u\":@\":\"☹️\",\n",
        "    u\">:(\":\"☹️\",\n",
        "    u\":'‑(\":\"😭\",\n",
        "    u\":'(\":\"😭\",\n",
        "    u\":'‑)\":\"😃\",\n",
        "    u\":')\":\"😃\",\n",
        "    u\"D‑':\":\"😧\",\n",
        "    u\"D:<\":\"😨\",\n",
        "    u\"D:\":\"😧\",\n",
        "    u\"D8\":\"😧\",\n",
        "    u\"D;\":\"😧\",\n",
        "    u\"D=\":\"😧\",\n",
        "    u\"DX\":\"😧\",\n",
        "    u\":‑O\":\"😮\",\n",
        "    u\":O\":\"😮\",\n",
        "    u\":‑o\":\"😮\",\n",
        "    u\":o\":\"😮\",\n",
        "    u\":-0\":\"😮\",\n",
        "    u\"8‑0\":\"😮\",\n",
        "    u\">:O\":\"😮\",\n",
        "    u\":-*\":\"😗\",\n",
        "    u\":*\":\"😗\",\n",
        "    u\":X\":\"😗\",\n",
        "    u\";‑)\":\"😉\",\n",
        "    u\";)\":\"😉\",\n",
        "    u\"*-)\":\"😉\",\n",
        "    u\"*)\":\"😉\",\n",
        "    u\";‑]\":\"😉\",\n",
        "    u\";]\":\"😉\",\n",
        "    u\";^)\":\"😉\",\n",
        "    u\":‑,\":\"😉\",\n",
        "    u\";D\":\"😉\",\n",
        "    u\":‑P\":\"😛\",\n",
        "    u\":P\":\"😛\",\n",
        "    u\"X‑P\":\"😛\",\n",
        "    u\"XP\":\"😛\",\n",
        "    u\":‑Þ\":\"😛\",\n",
        "    u\":Þ\":\"😛\",\n",
        "    u\":b\":\"😛\",\n",
        "    u\"d:\":\"😛\",\n",
        "    u\"=p\":\"😛\",\n",
        "    u\">:P\":\"😛\",\n",
        "    u\":‑/\":\"😕\",\n",
        "    u\":/\":\"😕\",\n",
        "    u\":-[.]\":\"😕\",\n",
        "    u\">:[(\\)]\":\"😕\",\n",
        "    u\">:/\":\"😕\",\n",
        "    u\":[(\\)]\":\"😕\",\n",
        "    u\"=/\":\"😕\",\n",
        "    u\"=[(\\)]\":\"😕\",\n",
        "    u\":L\":\"😕\",\n",
        "    u\"=L\":\"😕\",\n",
        "    u\":S\":\"😕\",\n",
        "    u\":‑|\":\"😐\",\n",
        "    u\":|\":\"😐\",\n",
        "    u\":$\":\"😳\",\n",
        "    u\":‑x\":\"🤐\",\n",
        "    u\":x\":\"🤐\",\n",
        "    u\":‑#\":\"🤐\",\n",
        "    u\":#\":\"🤐\",\n",
        "    u\":‑&\":\"🤐\",\n",
        "    u\":&\":\"🤐\",\n",
        "    u\"O:‑)\":\"😇\",\n",
        "    u\"O:)\":\"😇\",\n",
        "    u\"0:‑3\":\"😇\",\n",
        "    u\"0:3\":\"😇\",\n",
        "    u\"0:‑)\":\"😇\",\n",
        "    u\"0:)\":\"😇\",\n",
        "    u\":‑b\":\"😛\",\n",
        "    u\"0;^)\":\"😇\",\n",
        "    u\">:‑)\":\"😈\",\n",
        "    u\">:)\":\"😈\",\n",
        "    u\"}:‑)\":\"😈\",\n",
        "    u\"}:)\":\"😈\",\n",
        "    u\"3:‑)\":\"😈\",\n",
        "    u\"3:)\":\"😈\",\n",
        "    u\">;)\":\"😈\",\n",
        "    u\"|;‑)\":\"😎\",\n",
        "    u\"|‑O\":\"😏\",\n",
        "    u\":‑J\":\"😏\",\n",
        "    u\"%‑)\":\"😵\",\n",
        "    u\"%)\":\"😵\",\n",
        "    u\":-###..\":\"🤒\",\n",
        "    u\":###..\":\"🤒\",\n",
        "    u\"(>_<)\":\"😣\",\n",
        "    u\"(>_<)>\":\"😣\",\n",
        "    u\"(';')\":\"Baby\",\n",
        "    u\"(^^>``\":\"😓\",\n",
        "    u\"(^_^;)\":\"😓\",\n",
        "    u\"(-_-;)\":\"😓\",\n",
        "    u\"(~_~;) (・.・;)\":\"😓\",\n",
        "    u\"(-_-)zzz\":\"😴\",\n",
        "    u\"(^_-)\":\"😉\",\n",
        "    u\"((+_+))\":\"😕\",\n",
        "    u\"(+o+)\":\"😕\",\n",
        "    u\"^_^\":\"😃\",\n",
        "    u\"(^_^)/\":\"😃\",\n",
        "    u\"(^O^)／\":\"😃\",\n",
        "    u\"(^o^)／\":\"😃\",\n",
        "    u\"(__)\":\"🙇\",\n",
        "    u\"_(._.)_\":\"🙇\",\n",
        "    u\"<(_ _)>\":\"🙇\",\n",
        "    u\"<m(__)m>\":\"🙇\",\n",
        "    u\"m(__)m\":\"🙇\",\n",
        "    u\"m(_ _)m\":\"🙇\",\n",
        "    u\"('_')\":\"😭\",\n",
        "    u\"(/_;)\":\"😭\",\n",
        "    u\"(T_T) (;_;)\":\"😭\",\n",
        "    u\"(;_;\":\"😭\",\n",
        "    u\"(;_:)\":\"😭\",\n",
        "    u\"(;O;)\":\"😭\",\n",
        "    u\"(:_;)\":\"😭\",\n",
        "    u\"(ToT)\":\"😭\",\n",
        "    u\";_;\":\"😭\",\n",
        "    u\";-;\":\"😭\",\n",
        "    u\";n;\":\"😭\",\n",
        "    u\";;\":\"😭\",\n",
        "    u\"Q.Q\":\"😭\",\n",
        "    u\"T.T\":\"😭\",\n",
        "    u\"QQ\":\"😭\",\n",
        "    u\"Q_Q\":\"😭\",\n",
        "    u\"(-.-)\":\"😞\",\n",
        "    u\"(-_-)\":\"😞\",\n",
        "    u\"(一一)\":\"😞\",\n",
        "    u\"(；一_一)\":\"😞\",\n",
        "    u\"(=_=)\":\"😩\",\n",
        "    u\"(=^·^=)\":\"😺\",\n",
        "    u\"(=^··^=)\":\"😺\",\n",
        "    u\"=_^= \":\"😺\",\n",
        "    u\"(..)\":\"😔\",\n",
        "    u\"(._.)\":\"😔\",\n",
        "    u\"(・・?\":\"😕\",\n",
        "    u\"(?_?)\":\"😕\",\n",
        "    u\">^_^<\":\"😃\",\n",
        "    u\"<^!^>\":\"😃\",\n",
        "    u\"^/^\":\"😃\",\n",
        "    u\"（*^_^*）\" :\"😃\",\n",
        "    u\"(^<^) (^.^)\":\"😃\",\n",
        "    u\"(^^)\":\"😃\",\n",
        "    u\"(^.^)\":\"😃\",\n",
        "    u\"(^_^.)\":\"😃\",\n",
        "    u\"(^_^)\":\"😃\",\n",
        "    u\"(^^)\":\"😃\",\n",
        "    u\"(^J^)\":\"😃\",\n",
        "    u\"(*^.^*)\":\"😃\",\n",
        "    u\"(^—^）\":\"😃\",\n",
        "    u\"(#^.^#)\":\"😃\",\n",
        "    u\"（^—^）\":\"👋\",\n",
        "    u\"(;_;)/~~~\":\"👋\",\n",
        "    u\"(^.^)/~~~\":\"👋\",\n",
        "    u\"(-_-)/~~~ ($··)/~~~\":\"👋\",\n",
        "    u\"(T_T)/~~~\":\"👋\",\n",
        "    u\"(ToT)/~~~\":\"👋\",\n",
        "    u\"(*^0^*)\":\"😍\",\n",
        "    u\"(*_*)\":\"😍\",\n",
        "    u\"(*_*;\":\"😍\",\n",
        "    u\"(+_+) (@_@)\":\"😍\",\n",
        "    u\"(*^^)v\":\"😂\",\n",
        "    u\"(^_^)v\":\"😂\",\n",
        "    u'(-\"-)':\"😓\",\n",
        "    u\"(ーー;)\":\"😓\",\n",
        "    u\"(^0_0^)\":\"😎\",\n",
        "    u\"(＾ｖ＾)\":\"😀\",\n",
        "    u\"(＾ｕ＾)\":\"😀\",\n",
        "    u\"(^)o(^)\":\"😀\",\n",
        "    u\"(^O^)\":\"😀\",\n",
        "    u\"(^o^)\":\"😀\",\n",
        "    u\")^o^(\":\"😀\",\n",
        "    u\":O o_O\":\"😮\",\n",
        "    u\"o_0\":\"😮\",\n",
        "    u\"o.O\":\"😮\",\n",
        "    u\"(o.o)\":\"😮\",\n",
        "    u\"oO\":\"😮\",\n",
        "}\n",
        "\n",
        "\n",
        "def str2emoji(tweet):\n",
        "\tfor pos, ej in enumerate(tweet):\n",
        "\t\tif ej in emojis:\n",
        "\t\t\ttweet[pos] = emojis[ej]\n",
        "\treturn tweet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS54Je2R-zz1",
        "colab_type": "code",
        "outputId": "e4cbde9b-dfd2-4159-b6f9-8fa2b0779ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyJRHBxPvsqI",
        "colab_type": "code",
        "outputId": "18d8a55a-53a3-415f-c544-9c08ae808284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "from unidecode import unidecode\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "import string\n",
        "\n",
        "def tokenize(tweet):\n",
        "  tweet = re.sub(r\"\\\\u2019\", \"'\", tweet)\n",
        "  tweet = re.sub(r\"\\\\u002c\", \",\", tweet)\n",
        "  tweet=' '.join(str2emoji(unidecode(tweet).lower().split()))\n",
        "  tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
        "  tweet = re.sub(r\" can\\'t\", \" cannot\", tweet)\n",
        "  tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n",
        "  tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
        "  tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
        "  tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
        "  tweet = re.sub(r\"\\'s\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\'n\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
        "  tweet = re.sub(r\"@\\w+\", r' ',tweet)\n",
        "  tweet = re.sub(r\"#\\w+\", r' ',tweet)\n",
        "  tweet = re.sub(r\"[.]+\",\" \",tweet)\n",
        "  tweet = tknzr.tokenize(tweet)\n",
        "  #tweet = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v']  else lemmatizer.lemmatize(i) for i,j in pos_tag(tknzr.tokenize(tweet))]\n",
        "  tweet = [i for i in tweet if (i not in set(stopwords.words('english')) - set(['not', 'no'])) and (i not in string.punctuation) and i != '']\n",
        "  return tweet"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrKBkVD2w_Fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_3['tweet'] = df_test_3['tweet'].apply(tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL12OGREk7-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Takes time, please be patient\n",
        "df_train_3['tweet'] = df_train_3['tweet'].apply(tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpw-Y1QUk96u",
        "colab_type": "code",
        "outputId": "67a16adf-a3b2-4737-c091-cae31bcad038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train_7['tweet'] = df_train_7['tweet'].apply(tokenize)\n",
        "df_test_3.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>264221473558917120</td>\n",
              "      <td>0</td>\n",
              "      <td>[manchester, united, try, return, winning, way...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>264091690632105984</td>\n",
              "      <td>0</td>\n",
              "      <td>[going, bulls, game, aaliyah, hope, next, thur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>263929564907069441</td>\n",
              "      <td>0</td>\n",
              "      <td>[toon, fans, spare, ticket, anfield, sunday, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>263759328782204928</td>\n",
              "      <td>1</td>\n",
              "      <td>[louis, inspired, outfit, monday, zayn, inspir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>259546192722161664</td>\n",
              "      <td>0</td>\n",
              "      <td>[going, bed, rose, parade, game, tomorrow]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id  ...                                              tweet\n",
              "0  264221473558917120  ...  [manchester, united, try, return, winning, way...\n",
              "1  264091690632105984  ...  [going, bulls, game, aaliyah, hope, next, thur...\n",
              "2  263929564907069441  ...  [toon, fans, spare, ticket, anfield, sunday, w...\n",
              "3  263759328782204928  ...  [louis, inspired, outfit, monday, zayn, inspir...\n",
              "4  259546192722161664  ...         [going, bed, rose, parade, game, tomorrow]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riyCrLJrl5qc",
        "colab_type": "text"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsc998mESO8",
        "colab_type": "code",
        "outputId": "4fb74ac6-b119-463c-d27a-0802ff9e645a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Takes time, please be patient\n",
        "from gensim.models import KeyedVectors\n",
        "word2Vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-b8eecca0d46f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2Vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     return open(uri, mode, ignore_ext=ignore_extension,\n\u001b[0;32m--> 458\u001b[0;31m                 transport_params=transport_params, **scrubbed_kwargs)\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dBA6GuCIwBL",
        "colab_type": "code",
        "outputId": "5e33a0bb-adc3-4350-8dce-3cb08a402ad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(word2Vec['not'].reshape(1, -1), word2Vec['no'].reshape(1, -1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5200427]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98yjDAknsn4L",
        "colab_type": "code",
        "outputId": "ac007686-f3b8-4dd3-ac1e-aeac9aac91cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "def words_to_indexes(word2Vec, words):\n",
        "  ret = []\n",
        "  for w in words:\n",
        "    try:\n",
        "      ret.append(word2Vec[w])\n",
        "    except:\n",
        "      pass\n",
        "  return np.array(ret)\n",
        "\n",
        "words_to_indexes(word2Vec, ['I', 'love', 'paris']).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzCt7e2Kl86t",
        "colab_type": "text"
      },
      "source": [
        "### Download word values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTNAKQ0vOM2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Emolex\n",
        "import urllib.request\n",
        "\n",
        "filedata = urllib.request.urlopen('http://www.saifmohammad.com/WebDocs/NRC-AffectIntensity-Lexicon.txt')\n",
        "emolex = filedata.read().decode(\"utf-8\").split('\\n')\n",
        "\n",
        "for i, l in enumerate(emolex):\n",
        "  if l == 'term\\tscore\\tAffectDimension':\n",
        "    k = i\n",
        "    break\n",
        "\n",
        "emolex = emolex[k+1:-1]\n",
        "\n",
        "l_emotions = set()\n",
        "l_words = set()\n",
        "for i, l in enumerate(emolex):\n",
        "  l = l.split('\\t')\n",
        "  emolex[i] = l\n",
        "  emolex[i][1] = float(emolex[i][1])\n",
        "  l_words.add(l[0])\n",
        "  l_emotions.add(l[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1S9zMIIOhNk",
        "colab_type": "code",
        "outputId": "5eb13814-4e62-4f2e-b312-aca71ca34d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "emolex_df = pd.DataFrame(columns=['word'] + list(l_emotions))\n",
        "l_words = list(l_words)\n",
        "for i in range(len(l_words)):\n",
        "  emolex_df.loc[i] = [l_words[i]] + [0.0 for i in range(4)]\n",
        "emolex_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>fear</th>\n",
              "      <th>anger</th>\n",
              "      <th>sadness</th>\n",
              "      <th>joy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wages</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shameful</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>noworries</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>eternal</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>shatter</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        word  fear  anger  sadness  joy\n",
              "0      wages   0.0    0.0      0.0  0.0\n",
              "1   shameful   0.0    0.0      0.0  0.0\n",
              "2  noworries   0.0    0.0      0.0  0.0\n",
              "3    eternal   0.0    0.0      0.0  0.0\n",
              "4    shatter   0.0    0.0      0.0  0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_zc3dWoS7Jf",
        "colab_type": "code",
        "outputId": "8381b92a-ba71-4033-8553-703dcdab6f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "for l in emolex:\n",
        "  emolex_df.loc[emolex_df['word'] == l[0], l[2]] = l[1]\n",
        "emolex_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>fear</th>\n",
              "      <th>anger</th>\n",
              "      <th>sadness</th>\n",
              "      <th>joy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wages</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shameful</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.609</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>noworries</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>eternal</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>shatter</td>\n",
              "      <td>0.672</td>\n",
              "      <td>0.561</td>\n",
              "      <td>0.609</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        word   fear  anger  sadness    joy\n",
              "0      wages  0.000  0.000    0.000  0.169\n",
              "1   shameful  0.000  0.000    0.609  0.000\n",
              "2  noworries  0.000  0.000    0.000  0.453\n",
              "3    eternal  0.000  0.000    0.000  0.469\n",
              "4    shatter  0.672  0.561    0.609  0.000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnecggYMUrNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Positive/Negative words\n",
        "filedata = urllib.request.urlopen('https://raw.githubusercontent.com/shekhargulati/sentiment-analysis-python/master/opinion-lexicon-English/negative-words.txt')\n",
        "negative_words = filedata.read().decode(\"ISO-8859-1\").split('\\n')[35:-1]\n",
        "filedata = urllib.request.urlopen('https://raw.githubusercontent.com/shekhargulati/sentiment-analysis-python/master/opinion-lexicon-English/positive-words.txt')\n",
        "positive_words = filedata.read().decode(\"utf-8\").split('\\n')[35:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84D2dJ34WerV",
        "colab_type": "code",
        "outputId": "d52a150c-241a-47b3-a186-ad429fe414fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "negative_words[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2-faced',\n",
              " '2-faces',\n",
              " 'abnormal',\n",
              " 'abolish',\n",
              " 'abominable',\n",
              " 'abominably',\n",
              " 'abominate',\n",
              " 'abomination',\n",
              " 'abort',\n",
              " 'aborted']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svGEmmrNXaqY",
        "colab_type": "code",
        "outputId": "46c1885b-aada-4d56-b781-85d279dd581a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "positive_words[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a+',\n",
              " 'abound',\n",
              " 'abounds',\n",
              " 'abundance',\n",
              " 'abundant',\n",
              " 'accessable',\n",
              " 'accessible',\n",
              " 'acclaim',\n",
              " 'acclaimed',\n",
              " 'acclamation']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aacwXKymXeXD",
        "colab_type": "code",
        "outputId": "56566323-0f82-4c02-b2dd-f22179fffda4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Download Emoji Emotion\n",
        "import json\n",
        "filedata = urllib.request.urlopen('https://raw.githubusercontent.com/words/emoji-emotion/master/index.json')\n",
        "emojis_valence = json.load(filedata)\n",
        "emojis_valence[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'emoji': '💯', 'name': '100', 'polarity': 3},\n",
              " {'emoji': '😠', 'name': 'angry', 'polarity': -3},\n",
              " {'emoji': '😧', 'name': 'anguished', 'polarity': -3},\n",
              " {'emoji': '😲', 'name': 'astonished', 'polarity': 2},\n",
              " {'emoji': '🖤', 'name': 'black_heart', 'polarity': 3},\n",
              " {'emoji': '💙', 'name': 'blue_heart', 'polarity': 3},\n",
              " {'emoji': '😊', 'name': 'blush', 'polarity': 2},\n",
              " {'emoji': '💔', 'name': 'broken_heart', 'polarity': -3},\n",
              " {'emoji': '👏', 'name': 'clap', 'polarity': 3},\n",
              " {'emoji': '🤡', 'name': 'clown_face', 'polarity': 0}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixU2pE3zmKOs",
        "colab_type": "text"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9OLLB5JYjP3",
        "colab_type": "code",
        "outputId": "65ab409a-3fea-40ae-db3a-55ca9740d946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# Padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "W2V_LENGTH = len(word2Vec['toto'])\n",
        "MAX_SEQUENCE_LENGTH = max([len(x) for x in df_train_3['tweet']])\n",
        "\n",
        "X_train = [words_to_indexes(word2Vec, x) for x in df_train_3['tweet']]\n",
        "y_train = df_train_3['sentiment']\n",
        "\n",
        "\n",
        "#X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc4668961e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mW2V_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2Vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'toto'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word2Vec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBeAGITMvXud",
        "colab_type": "code",
        "outputId": "0cd2091a-2617-44f3-ee9b-e77f653135c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt-78faTuxRQ",
        "colab_type": "code",
        "outputId": "2cd11c43-6088-4b04-cdcc-05040114fb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "pad_sequences(X_train, maxlen=20).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a94262e541bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lnBfMQEvr4u",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ7776nlbGzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(INPUT_LENGTH, 1)))\n",
        "model.add(Dense(13, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQ0EoidbVur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)], verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL9ohYBzbWj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYXHxNNbivKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "4cfed563-9e72-4598-94c6-7dec5914f2f6"
      },
      "source": [
        "from collections import Counter\n",
        "#from preprocessing import standardization, data_preprocessing\n",
        "import sys\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential, Model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU\n",
        "import matplotlib as mpl\n",
        "from keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import numpy\n",
        "\n",
        "#mpl.use('TkAgg')  # or whatever other backend that you want\n",
        "#import matplotlib.pyplot as plt\n",
        "np.random.seed(7)\n",
        "from keras.models import load_model\n",
        "\n",
        "from preprocessing import data_preprocessing, data_preprocessing_test\n",
        "\n",
        "EMBEDDING_FILE=\"/home/abdou/Téléchargements/GoogleNews-vectors-negative300.bin\"\n",
        "EMBEDDING_DIM=300\n",
        "\n",
        "corpora_train_3=\"/home/abdou/Documents/TP_transfer_learning_2018/data/task_A/data_train_3.csv\"\n",
        "corpora_train_7=\"/home/abdou/Documents/TP_transfer_learning_2018/data/task_A/data_train_7.csv\"\n",
        "corpora_test_7=\"/home/abdou/Documents/TP_transfer_learning_2018/data/task_A/data_test_7.csv\"\n",
        "\n",
        "\n",
        "\n",
        "tweets_train_3, sentiments_train_3 =  data_preprocessing(corpora_train_3,'train')\n",
        "tweets_train_7, sentiments_train_7 =  data_preprocessing(corpora_train_7,'test')\n",
        "\n",
        "all_tweet = tweets_train_3.append(tweets_train_7)\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(filters=' ')\n",
        "tokenizer.fit_on_texts(all_tweet)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "\n",
        "sequences_train_3 = tokenizer.texts_to_sequences(tweets_train_3)\n",
        "sequences_train_7 = tokenizer.texts_to_sequences(tweets_train_7)\n",
        "sequences = sequences_train_3 + sequences_train_7\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 0\n",
        "for elt in sequences:\n",
        "\tif len(elt) > MAX_SEQUENCE_LENGTH:\n",
        "\t\tMAX_SEQUENCE_LENGTH = len(elt)\n",
        "\n",
        "print(MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "data_train_3 = pad_sequences(sequences_train_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_train_7 = pad_sequences(sequences_train_7, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "indices_train_3 = np.arange(data_train_3.shape[0])\n",
        "data_train_3 = data_train_3[indices_train_3]\n",
        "\n",
        "indices_train_7 = np.arange(data_train_7.shape[0])\n",
        "data_train_7 = data_train_7[indices_train_7]\n",
        "\n",
        "labels_train_3 = to_categorical(np.asarray(sentiments_train_3), 3)\n",
        "labels_train_3 = labels_train_3[indices_train_3]\n",
        "\n",
        "\n",
        "nb_words=len(word_index)+1\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "oov=[]\n",
        "oov.append((np.random.rand(EMBEDDING_DIM) * 2.0) - 1.0)\n",
        "oov = oov / np.linalg.norm(oov)\n",
        "\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec.vocab:\n",
        "        embedding_matrix[i] = word2vec.word_vec(word)\n",
        "    else:\n",
        "        embedding_matrix[i] = oov\n",
        "\n",
        "\n",
        "split_idx = int(len(data_train_3)*0.70)\n",
        "x_train_3, x_val_3 = data_train_3[:split_idx], data_train_3[split_idx:]\n",
        "y_train_3, y_val_3 = labels_train_3 [:split_idx], labels_train_3[split_idx:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('training set: ' + str(len(x_train_3)) + ' samples')\n",
        "print('validation set: ' + str(len(x_val_3)) + ' samples')\n",
        "# print('test set: ' + str(len(x_test)) + ' samples')\n",
        "\n",
        "\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False, name='embedding_layer')\n",
        "\n",
        "\n",
        "print('x_train', x_train_3.shape)\n",
        "print('y_train', y_train_3.shape)\n",
        "\n",
        "\n",
        "def model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer):\n",
        "\tmodel1 = Sequential()\n",
        "\tmodel1.add(embedding_layer)\n",
        "\tmodel1.add(LSTM(32))\n",
        "\tmodel1.add(Dropout(0.2))\n",
        "\tmodel1.add(Dense(32, activation='relu'))\n",
        "\tmodel1.add(Dropout(0.2))\n",
        "\tmodel1.add(Dense(3, activation='softmax'))\n",
        "\tmodel1.compile(loss='categorical_crossentropy',\n",
        "\t\t\t      optimizer='Adam',\n",
        "\t\t\t      metrics=['acc'])\n",
        "\tmodel1.summary()\n",
        "\thistory=model1.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3), epochs=6, batch_size=50)\n",
        "\tmodel1.save(\"./model1.h5\")\n",
        "\n",
        "\n",
        "model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer)\n",
        "\n",
        "\n",
        "# ========================================================================\n",
        "\n",
        "\n",
        "labels_train_7 = to_categorical(np.asarray(sentiments_train_7), 7)\n",
        "labels_train_7 = labels_train_7[indices_train_7]\n",
        "\n",
        "\n",
        "split_idx = int(len(data_train_7)*0.85\n",
        ")\n",
        "x_train_7, x_val_7 = data_train_7[:split_idx], data_train_7[split_idx:]\n",
        "y_train_7, y_val_7 = labels_train_7 [:split_idx], labels_train_7[split_idx:]\n",
        "\n",
        "\n",
        "print('x_train', x_train_7.shape)\n",
        "print('y_train', y_train_7.shape)\n",
        "\n",
        "#, y_train_7, x_val_7,y_train_7)\n",
        "\n",
        "model=load_model(\"./model1.h5\")\n",
        "model.summary()\n",
        "model.layers.pop()\n",
        "model.layers.pop()\n",
        "#model.outputs = [model.layers[-1].output]\n",
        "model.add(Dense(150,activation='relu',name='dense1'))\n",
        "model.add(Dense(64,activation='relu',name='dense2'))\n",
        "model.add(Dense(7,activation='softmax',name='dense3'))\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "history = model.fit(x_train_7, y_train_7,   validation_data=(x_val_7,y_val_7), epochs=11, batch_size=50)\n",
        "model.save(\"./model3.h5\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-49ec824d73ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_preprocessing_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/abdou/Téléchargements/GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'data_preprocessing'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjevz_mRqCFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}